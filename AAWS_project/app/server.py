# ë¦¬ëˆ…ìŠ¤ í™˜ê²½ì—ì„œ ì‹œìŠ¤í…œ SQLite ë²„ì „ì´ ë‚®ì„ ê²½ìš° pysqlite3ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•˜ë„ë¡ ê°•ì œí•¨
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

import sys
import os
from dotenv import load_dotenv

# 1. Setup Project Root Path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# 2. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ ëª…ì‹œì  ì§€ì •)
dotenv_path = os.path.join(project_root, ".env")
if os.path.exists(dotenv_path):
    print(f"Loading .env from: {dotenv_path}")
    load_dotenv(dotenv_path)
    
    # LangSmith Project Setting (Server Specific)
    # .envì—ëŠ” API Keyë§Œ ìˆê³ , í”„ë¡œì íŠ¸ëª…ì€ ì—¬ê¸°ì„œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    os.environ["LANGSMITH_TRACING"] = "true"
    os.environ["LANGSMITH_PROJECT"] = "llmops-agent-server"
    print(f"ğŸ“ˆ LangSmith Tracing Enabled. Project: {os.environ['LANGSMITH_PROJECT']}")
else:
    print("Warning: .env file not found.")

import logging
import json
import traceback
from typing import AsyncGenerator, Optional, Dict, Any

from fastapi import FastAPI, APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

# --- Agent Executors Import ---
# ì£¼ì˜: í™˜ê²½ ë³€ìˆ˜(API Key)ê°€ ë¡œë“œëœ í›„ì— ì—ì´ì „íŠ¸ ëª¨ë“ˆì„ ì„í¬íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤.
from app.agents.chatbot import agent_executor as chatbot_agent
from app.agents.multimodal_agent import agent_executor as multimodal_agent
from app.agents.navigator_agent import agent_executor as navigator_agent
from app.agents.coder_agent import agent_executor as coder_agent

# Logging Setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("LLMOps_Server")

# --- Schemas ---
class UserInput(BaseModel):
    message: str
    thread_id: Optional[str] = None

class StreamInput(UserInput):
    stream_tokens: bool = Field(default=True)

class ChatMessage(BaseModel):
    type: str
    content: str


# --- Router Factory ---
def create_agent_router(agent_executor, prefix: str, tags: list = None) -> APIRouter:
    """
    ì£¼ì–´ì§„ ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸°(Executor)ë¥¼ ìœ„í•œ FastAPI ë¼ìš°í„°ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    /invoke ë° /stream ì—”ë“œí¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë“±ë¡í•©ë‹ˆë‹¤.
    """
    router = APIRouter(prefix=prefix, tags=tags or [prefix])

    async def _stream_generator(input_data: StreamInput) -> AsyncGenerator[str, None]:
        try:
            config = {"configurable": {"thread_id": input_data.thread_id}} if input_data.thread_id else {}
            
            # LangGraph astream_events (v2)
            async for event in agent_executor.astream_events(
                {"messages": [("user", input_data.message)]}, 
                config=config,
                version="v2"
            ):
                kind = event["event"]
                
                # Tool Start
                if kind == "on_tool_start":
                    yield f"data: {json.dumps({'type': 'tool_start', 'name': event['name'], 'input': event['data'].get('input')})}\n\n"
                
                # Token Streaming (Chat Model)
                elif kind == "on_chat_model_stream":
                    # ë‚´ë¶€ ë¡œì§(ì˜ˆ: Self-Query êµ¬ì„± ë“±)ì—ì„œ ë°œìƒí•˜ëŠ” ì¤‘ê°„ ë‹¨ê³„ì˜ í† í°ì€ ì œì™¸í•©ë‹ˆë‹¤.
                    tags = event.get("tags", [])
                    if "exclude_from_stream" in tags:
                        continue

                    chunk = event["data"]["chunk"]
                    if chunk and chunk.content:
                        yield f"data: {json.dumps({'type': 'token', 'content': chunk.content})}\n\n"

        except Exception as e:
            logger.error(f"Stream error in {prefix}: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
        
        yield "event: end\ndata: \n\n"

    @router.post("/invoke", response_model=ChatMessage)
    async def invoke(input_data: UserInput):
        try:
            config = {"configurable": {"thread_id": input_data.thread_id}} if input_data.thread_id else {}
            
            # invoke returns the final state
            result = await agent_executor.ainvoke(
                {"messages": [("user", input_data.message)]},
                config=config
            )
            # LangGraph: State['messages'][-1] is the AI response
            last_message = result["messages"][-1]
            return ChatMessage(type="ai", content=last_message.content)
        except Exception as e:
            logger.error(f"Invocation error in {prefix}: {e}")
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=str(e))

    @router.post("/stream")
    async def stream(input_data: StreamInput):
        return StreamingResponse(
            _stream_generator(input_data), 
            media_type="text/event-stream"
        )
        
    return router


# --- App Initialization ---
app = FastAPI(
    title="LLMOps Class Agent Server", 
    version="1.0",
    description="Unified Server for Multiple Agents"
)

@app.get("/health")
def health():
    return {"status": "ok", "agents": ["chatbot_agent", "multimodal_agent"]}

# --- Register Routers ---
app.include_router(create_agent_router(chatbot_agent, "/chatbot", ["Chatbot"]))
app.include_router(create_agent_router(multimodal_agent, "/multimodal_agent", ["Multimodal"])) #ë’¤ì˜ ë¦¬ìŠ¤íŠ¸ ë¶€ë¶„ì€ API ë¬¸ì„œì—ì„œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸ëª…
app.include_router(create_agent_router(navigator_agent, "/navigator_agent", ["Web Navigator"]))
app.include_router(create_agent_router(coder_agent, "/coder_agent", ["Coder"]))  # coder agent ì¶”ê°€
app.include_router(create_agent_router(navigator_agent, "/navigator_agent", ["Web Navigator"]))

if __name__ == "__main__":
    import uvicorn
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=8000, help="Server Port")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Server Host")
    args = parser.parse_args()
    
    print(f"ğŸš€ Server starting on http://{args.host}:{args.port}")
    uvicorn.run(app, host=args.host, port=args.port)
