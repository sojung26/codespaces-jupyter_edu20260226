# ë¦¬ëˆ…ìŠ¤ í™˜ê²½ì—ì„œ ì‹œìŠ¤í…œ SQLite ë²„ì „ì´ ë‚®ì„ ê²½ìš° pysqlite3ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•˜ë„ë¡ ê°•ì œí•¨
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

import sys
import os
from dotenv import load_dotenv

# 1. Setup Project Root Path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# 2. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ ëª…ì‹œì  ì§€ì •)
dotenv_path = os.path.join(project_root, ".env")
if os.path.exists(dotenv_path):
    print(f"Loading .env from: {dotenv_path}")
    load_dotenv(dotenv_path)
    
    # LangSmith Project Setting (Server Specific)
    # .envì—ëŠ” API Keyë§Œ ìˆê³ , í”„ë¡œì íŠ¸ëª…ì€ ì—¬ê¸°ì„œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    os.environ["LANGSMITH_TRACING"] = "true"
    os.environ["LANGSMITH_PROJECT"] = "llmops-agent-server"
    print(f"ğŸ“ˆ LangSmith Tracing Enabled. Project: {os.environ['LANGSMITH_PROJECT']}")
else:
    print("Warning: .env file not found.")

import logging
import json
import traceback
from typing import AsyncGenerator, Optional, Dict, Any

from fastapi import FastAPI, APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

import importlib

# Logging Setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("LLMOps_Server")

# --- Schemas ---
class UserInput(BaseModel):
    message: str
    thread_id: Optional[str] = None

class StreamInput(UserInput):
    stream_tokens: bool = Field(default=True)

class ChatMessage(BaseModel):
    type: str
    content: str

# --- Router Factory ---
def create_agent_router(agent_executor, prefix: str, tags: list = None) -> APIRouter:
    """
    ì£¼ì–´ì§„ ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸°(Executor)ë¥¼ ìœ„í•œ FastAPI ë¼ìš°í„°ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    /invoke ë° /stream ì—”ë“œí¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë“±ë¡í•©ë‹ˆë‹¤.
    """
    router = APIRouter(prefix=prefix, tags=tags or [prefix])

    async def _stream_generator(input_data: StreamInput) -> AsyncGenerator[str, None]:
        try:
            config = {"configurable": {"thread_id": input_data.thread_id}} if input_data.thread_id else {}
            
            # LangGraph astream_events (v2)
            async for event in agent_executor.astream_events(
                {"messages": [("user", input_data.message)]}, 
                config=config,
                version="v2"
            ):
                kind = event["event"]
                
                # Tool Start
                if kind == "on_tool_start":
                    yield f"data: {json.dumps({'type': 'tool_start', 'name': event['name'], 'input': event['data'].get('input')})}\n\n"
                
                # Token Streaming (Chat Model)
                elif kind == "on_chat_model_stream":
                    # ë‚´ë¶€ ë¡œì§(ì˜ˆ: Self-Query êµ¬ì„± ë“±)ì—ì„œ ë°œìƒí•˜ëŠ” ì¤‘ê°„ ë‹¨ê³„ì˜ í† í°ì€ ì œì™¸í•©ë‹ˆë‹¤.
                    tags = event.get("tags", [])
                    if "exclude_from_stream" in tags:
                        continue

                    chunk = event["data"]["chunk"]
                    if chunk and chunk.content:
                        # [Gemini ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ ë°©ì–´ íŒŒì„œ]
                        raw_content = chunk.content
                        if isinstance(raw_content, list):
                            content_str = "".join([c.get("text", "") if isinstance(c, dict) else str(c) for c in raw_content])
                        else:
                            content_str = str(raw_content)

                        if content_str:
                            yield f"data: {json.dumps({'type': 'token', 'content': content_str})}\n\n"

        except Exception as e:
            logger.error(f"Stream error in {prefix}: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
        
        yield "event: end\ndata: \n\n"

    @router.post("/invoke", response_model=ChatMessage)
    async def invoke(input_data: UserInput):
        try:
            config = {"configurable": {"thread_id": input_data.thread_id}} if input_data.thread_id else {}
            
            # invoke returns the final state
            result = await agent_executor.ainvoke(
                {"messages": [("user", input_data.message)]},
                config=config
            )
            # LangGraph: State['messages'][-1] is the AI response
            last_message = result["messages"][-1]
            
            # [Gemini ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ ë°©ì–´ íŒŒì„œ]
            raw_content = last_message.content
            if isinstance(raw_content, list):
                content_str = "".join([c.get("text", "") if isinstance(c, dict) else str(c) for c in raw_content])
            else:
                content_str = str(raw_content)
                
            return ChatMessage(type="ai", content=content_str)
        except Exception as e:
            logger.error(f"Invocation error in {prefix}: {e}")
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=str(e))

    @router.post("/stream")
    async def stream(input_data: StreamInput):
        return StreamingResponse(
            _stream_generator(input_data), 
            media_type="text/event-stream"
        )
        
    return router

# --- App Initialization ---
app = FastAPI(
    title="LLMOps Class Agent Server", 
    version="1.0",
    description="Unified Server for Multiple Agents"
)

# --- Dynamic Agent Loading & Router Registration ---
loaded_agents = []
agents_dir = os.path.join(project_root, "app", "agents")

if os.path.exists(agents_dir):
    for filename in os.listdir(agents_dir):
        if filename.endswith(".py") and filename != "__init__.py":
            agent_name = filename[:-3]
            module_path = f"app.agents.{agent_name}"
            
            try:
                # ë™ì ìœ¼ë¡œ ëª¨ë“ˆ ì„í¬íŠ¸
                module = importlib.import_module(module_path)
                
                # ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸°(executor) ê°ì²´ ì°¾ê¸°
                # ê´€ë¡€ìƒ agent_executor ì´ë¦„ ê¶Œì¥. ì—†ìœ¼ë©´ ëª¨ë“ˆëª…_agent í™•ì¸
                executor = getattr(module, "agent_executor", None)
                if not executor:
                    executor = getattr(module, f"{agent_name}_agent", None)
                    
                if executor:
                    logger.info(f"âœ… Loaded agent: {agent_name} from {module_path}")
                    tag_name = agent_name.replace("_", " ").title()
                    app.include_router(
                        create_agent_router(executor, f"/{agent_name}", [tag_name])
                    )
                    loaded_agents.append(agent_name)
                else:
                    logger.info(f"âš ï¸ Skip: {module_path} ëª¨ë“ˆì—ëŠ” 'agent_executor'ê°€ ì—†ì–´ ë¼ìš°í„° ë§¤í•‘ì„ ìƒëµí•©ë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"âŒ Failed to load agent {agent_name}: {e}")
                traceback.print_exc()

@app.get("/health")
def health():
    return {"status": "ok", "agents": loaded_agents}

if __name__ == "__main__":
    import uvicorn
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=8000, help="Server Port")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Server Host")
    args = parser.parse_args()
    
    print(f"ğŸš€ Server starting on http://{args.host}:{args.port}")
    uvicorn.run(app, host=args.host, port=args.port)