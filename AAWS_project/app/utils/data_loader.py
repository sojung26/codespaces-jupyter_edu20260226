import os
import os
from typing import List, Optional

from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_classic.retrievers.self_query.base import SelfQueryRetriever
from langchain_classic.chains.query_constructor.schema import AttributeInfo


# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

# ê²½ë¡œ ì„¤ì • (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DATA_DIR = os.path.join(BASE_DIR, "data")
CHROMA_DB_DIR = os.path.join(DATA_DIR, "chroma_db")
PDF_SOURCE_DIR = os.path.join(DATA_DIR, "bok_major_industry_reports")

# ì „ì—­ ë³€ìˆ˜ë¡œ Retriever ê´€ë¦¬ (Lazy Loading)
_retrievers = {
    "basic": None,
    "self_query": None,
    "multimodal": None
}

def _get_embedding_model():
    return OpenAIEmbeddings(model="text-embedding-3-large")

def _initialize_vectorstore(collection_name: str) -> Chroma:
    """
    ì§€ì •ëœ Collection Nameìœ¼ë¡œ Chroma VectorStoreë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒì„±í•©ë‹ˆë‹¤.
    (ì‹¤ìŠµ í™˜ê²½ì—ì„œëŠ” ì´ë¯¸ ìƒì„±ë˜ì—ˆë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜, ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„)
    """
    embedding_model = _get_embedding_model()
    
    # DB ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„± (í´ë”ë§Œ)
    if not os.path.exists(CHROMA_DB_DIR):
        os.makedirs(CHROMA_DB_DIR, exist_ok=True)

    print(f"ğŸ“‚ [DataLoader] Loading VectorStore: {collection_name}")
    vectorstore = Chroma(
        persist_directory=CHROMA_DB_DIR,
        embedding_function=embedding_model,
        collection_name=collection_name
    )
    return vectorstore

# --- 1. Basic RAG Retriever ---
def get_basic_retriever():
    if _retrievers["basic"]:
        return _retrievers["basic"]

    vectorstore = _initialize_vectorstore("basic_rag")
    # Basic RAGëŠ” ì¼ë°˜ì ì¸ Similarity Searchë¥¼ ì‚¬ìš©í•˜ëŠ” Retriever
    _retrievers["basic"] = vectorstore.as_retriever()
    return _retrievers["basic"]

# --- 2. Self-Query Retriever (Shared Logic) ---
def _create_self_query_retriever(collection_name: str):
    vectorstore = _initialize_vectorstore(collection_name)
    
    metadata_field_info = [
        AttributeInfo(
            name="year",
            description="The year of the report (e.g., 2024). Must be an integer.",
            type="integer",
        ),
        AttributeInfo(
            name="quarter",
            description="The quarter of the report. One of 1, 2, 3, 4. Must be an integer.",
            type="integer",
        ),
    ]
    document_content_description = "Bank of Korea Industry Reports"
    
    # Self-Queryë¥¼ ìœ„í•œ LLM (êµ¬ì¡°í™”ëœ ì¿¼ë¦¬ ìƒì„±ìš©)
    llm = ChatOpenAI(model="gpt-4o", temperature=0).with_config({"tags": ["exclude_from_stream"]})
    
    retriever = SelfQueryRetriever.from_llm(
        llm,
        vectorstore,
        document_content_description,
        metadata_field_info,
        verbose=False
    )
    return retriever

def get_self_query_retriever():
    if _retrievers["self_query"]:
        return _retrievers["self_query"]
        
    print("ğŸ›  [DataLoader] Initializing Self-Query Retriever...")
    _retrievers["self_query"] = _create_self_query_retriever("self_query")
    return _retrievers["self_query"]

def get_multimodal_retriever():
    if _retrievers["multimodal"]:
        return _retrievers["multimodal"]

    print("ğŸ›  [DataLoader] Initializing Multimodal Retriever (Self-Query enabled)...")
    _retrievers["multimodal"] = _create_self_query_retriever("multimodal")
    return _retrievers["multimodal"]


